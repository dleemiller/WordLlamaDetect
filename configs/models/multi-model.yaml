# Example configuration for concatenating multiple models
# Combined hidden_dim = 2304 + 896 = 3200
models:
  - name: "google/gemma-3-2b"
    type: "gemma3"
    hidden_dim: 2304
    shard_pattern: "model-*.safetensors"
    embedding_layer_name: "model.embed_tokens.weight"

  - name: "Qwen/Qwen2.5-0.5B"
    type: "qwen3"
    hidden_dim: 896
    shard_pattern: "model-*.safetensors"
    embedding_layer_name: "model.embed_tokens.weight"

languages:
  # Common languages - adjust based on your use case
  en: 0   # English
  es: 1   # Spanish
  fr: 2   # French
  de: 3   # German
  it: 4   # Italian
  pt: 5   # Portuguese
  nl: 6   # Dutch
  pl: 7   # Polish
  ru: 8   # Russian
  ja: 9   # Japanese
  zh: 10  # Chinese
  ko: 11  # Korean
  ar: 12  # Arabic
  hi: 13  # Hindi
  tr: 14  # Turkish

inference:
  max_sequence_length: 512
  pooling: "max"
