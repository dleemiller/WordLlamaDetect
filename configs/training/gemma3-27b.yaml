model_config_path: "configs/models/gemma3-27b.yaml"

dataset:
  name: "laurievb/OpenLID-v2"
  train_split: "train"
  val_split: "validation"
  test_split: "test"
  filter_languages: true  # Filter to only model's supported languages (all 204)
  #max_samples_per_language: 5000  # Balance dataset - 5000 samples per language
  shuffle_seed: 42  # Shuffle training split reproducibly (set to null to disable)

training:
  batch_size: 1536
  learning_rate: 0.002
  epochs: 2
  optimizer: "adamw"
  loss: "focal"     # Options: "cross_entropy", "focal"
  label_smoothing: 0      # Smoothing factor for CE/Focal (0 disables)
  focal_gamma: 10.0          # Focal loss gamma (only used when loss="focal")
  #focal_alpha: null         # Optional class weighting for focal loss (float or list)
  weight_decay: 0.0001
  gradient_clip: 1.0
  momentum: 0.9
  num_workers: 4

  # Class weighting
  class_weights: "log_population"
  class_weights_power: 0.75
  population_data_path: "configs/language_populations.yaml"


  # Learning rate scheduler
  scheduler: "cosine_warmup"  # Options: "none", "cosine", "cosine_warmup"
  warmup_steps: 5000           # Warmup steps (for cosine_warmup)
  min_lr_ratio: 0.1           # Min LR = learning_rate Ã— min_lr_ratio

  projection:
    dropout: 0.2

evaluation:
  metrics:
    - accuracy
    - f1_macro
    - f1_weighted
    - confusion_matrix
  flores_eval_every_steps: 1000   # Run FLORES-200 eval every N steps (null to disable)
  flores_split: "dev"
  flores_batch_size: null         # Defaults to training batch size when null
  flores_hf_dataset: "openlanguagedata/flores_plus"
  flores_cache_dir: null          # Optional cache dir for HF dataset

output:
  artifacts_dir: "artifacts/gemma3-27b/"
  checkpoint_dir: "artifacts/gemma3-27b/checkpoints/"
  checkpoint_every_steps: 5000   # Save checkpoint every N steps (null to disable)
  projection_matrix_name: "projection.safetensors"
  config_name: "model_config.yaml"
