model_config_path: "configs/models/gemma3-2b.yaml"

dataset:
  name: "laurievb/OpenLID-v2"
  train_split: "train"
  val_split: "validation"
  test_split: "test"
  filter_languages: true  # Filter to only model's supported languages
  max_samples_per_language: null  # Optional: set to balance dataset (e.g., 10000)
  shuffle_seed: 42  # Shuffle training split reproducibly (set to null to disable)

training:
  batch_size: 32
  learning_rate: 0.001
  epochs: 10
  optimizer: "adam"
  loss: "cross_entropy"     # Options: "cross_entropy", "focal"
  focal_gamma: 2.0          # Focal loss gamma (only used when loss="focal")
  focal_alpha: null         # Optional class weighting for focal loss (float or list)
  weight_decay: 0.00001
  gradient_clip: 1.0
  num_workers: 4

  # Learning rate scheduler
  scheduler: "cosine_warmup"  # Options: "none", "cosine", "cosine_warmup"
  warmup_steps: 500           # Warmup steps (for cosine_warmup)
  min_lr_ratio: 0.1           # Min LR = learning_rate Ã— min_lr_ratio

  projection:
    dropout: 0.1

evaluation:
  metrics:
    - accuracy
    - f1_macro
    - f1_weighted
    - confusion_matrix
  flores_eval_every_steps: null   # Run FLORES-200 eval every N steps (null to disable)
  flores_split: "dev"
  flores_batch_size: null         # Defaults to training batch size when null
  flores_hf_dataset: "openlanguagedata/flores_plus"
  flores_cache_dir: null          # Optional cache dir for HF dataset

output:
  artifacts_dir: "artifacts/"
  checkpoint_dir: "artifacts/checkpoints/"
  checkpoint_every_steps: 1000   # Save checkpoint every N steps (null to disable)
  projection_matrix_name: "projection.safetensors"
  config_name: "model_config.yaml"
